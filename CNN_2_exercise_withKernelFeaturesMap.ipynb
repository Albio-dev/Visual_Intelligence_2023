{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6CXLUOYS6C0"
   },
   "source": [
    "# <center>CNN 2D for image classification<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTLoLYbRl7Mr"
   },
   "source": [
    "Firstly:<br>\n",
    "\n",
    "<ol>\n",
    "  <li>Go to Edit\n",
    "  <li>Notebook settings\n",
    "  <li>On hardware accelerator, set GPU\n",
    "  <li>Save\n",
    "</ol>\n",
    "\n",
    "In this way we are able to use the free GPU available on Google Colab to train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC5fKN20TnHP"
   },
   "source": [
    "## Load drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SaqyUcbvTmnt",
    "outputId": "cd047fad-1642-4ec2-e19d-36e92d4eb1c1"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/IV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZNCsLaCTsnE"
   },
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6kXIF9vhS3C3"
   },
   "outputs": [],
   "source": [
    "# Install missing packages\n",
    "\n",
    "# Libraries\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import CustomDataset, compute_metrics, plot_weights, visTensor\n",
    "\n",
    "# Import the model\n",
    "from CNN_128x128 import CNN_128x128\n",
    "\n",
    "# Style for chart\n",
    "sns.set_style('darkgrid')\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=14)\n",
    "plt.rc('xtick', labelsize=13)\n",
    "plt.rc('ytick', labelsize=13)\n",
    "plt.rc('legend', fontsize=13)\n",
    "plt.rc('font', size=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHFZwfgnMf3O"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQdieoepNS4A"
   },
   "source": [
    "labels:\n",
    "* dogs = 0\n",
    "* flowers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0WfISlhZNIHd"
   },
   "outputs": [],
   "source": [
    "# Define train and test labels\n",
    "train_labels = np.zeros(2400)\n",
    "train_labels[1200:2400] = 1\n",
    "test_labels = np.zeros(800)\n",
    "test_labels[400:800] = 1\n",
    "train_labels = train_labels.astype('uint8')\n",
    "test_labels = test_labels.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "v4Trq3d5LzTN"
   },
   "outputs": [],
   "source": [
    "# Load train set\n",
    "train_data = [cv2.imread(file) for file in glob.glob('./Data/train/dog/*.jpg')]\n",
    "train_data.extend(cv2.imread(file) for file in glob.glob('./Data/train/flower/*.jpg'))\n",
    "\n",
    "# Load test set\n",
    "test_data = [cv2.imread(file) for file in glob.glob('./Data/test/dog/*.jpg')]\n",
    "test_data.extend(cv2.imread(file) for file in glob.glob('./Data/test/flower/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0s--ENNqjG0X"
   },
   "outputs": [],
   "source": [
    "#Â Random shuffle train and test set\n",
    "train_list = list(zip(train_data,train_labels))\n",
    "test_list = list(zip(test_data,test_labels))\n",
    "\n",
    "random.shuffle(train_list)\n",
    "random.shuffle(test_list)\n",
    "\n",
    "train_data, train_labels = zip(*train_list)\n",
    "test_data, test_labels = zip(*test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4Kr6_sDf8dr"
   },
   "source": [
    "### Create datasets for train and test & Define useful variables for deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdlUx4dIgA5S",
    "outputId": "e9810ca7-e5f7-4cee-d74a-fdfbafc585fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device where to run the model. GPU if available, otherwise cpu (very slow with deep learning models)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device: ',device)\n",
    "\n",
    "# Create Dataloader with batch size = 64\n",
    "train_dataset = CustomDataset(train_data,train_labels)    # we use a custom dataset defined in utils.py file\n",
    "test_dataset = CustomDataset(test_data,test_labels)       # we use a custom dataset defined in utils.py file\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainset = DataLoader(train_dataset,batch_size=batch_size,drop_last=True)    # construct the trainset with subjects divided in mini-batch\n",
    "testset = DataLoader(test_dataset,batch_size=batch_size,drop_last=True)      # construct the testset with subjects divided in mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "TaUnBPjTpKVD"
   },
   "outputs": [],
   "source": [
    "# Define useful variables\n",
    "models_trained_path = '/content/drive/MyDrive/IV/models_trained/'\n",
    "if not os.path.exists(models_trained_path):                 # create a directory where to save the best model\n",
    "    os.makedirs(models_trained_path)\n",
    "\n",
    "best_acc = 0.0\n",
    "num_epochs = 40                                   # number of epochs\n",
    "lr = 0.01                                         # learning rate\n",
    "n_classes = len(np.unique(train_labels))                # number of classes in the dataset\n",
    "lab_classes = ['Dog','Flower']\n",
    "\n",
    "# Variables to store the resuts\n",
    "losses = []\n",
    "acc_train = []\n",
    "pred_label_train = torch.empty((0)).to(device)    # .to(device) to move the data/model on GPU or CPU (default)\n",
    "true_label_train = torch.empty((0)).to(device)\n",
    "\n",
    "# Model\n",
    "model = CNN_128x128(input_channel=3,num_classes=n_classes).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optim = torch.optim.SGD(model.parameters(),lr = lr, momentum=0.5)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETBubCZsqayE"
   },
   "source": [
    "### Train the 1D CNN to classify ECG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QoamR6eKqf8n",
    "outputId": "a9a405e5-7fe8-4ca7-f87d-84c1922696af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch : 1/40, loss = 0.5702 - acc = 0.5853\n",
      "  epoch : 2/40, loss = 0.5304 - acc = 0.6719\n",
      "  epoch : 3/40, loss = 0.5239 - acc = 0.6981\n",
      "  epoch : 4/40, loss = 0.4931 - acc = 0.7116\n",
      "  epoch : 5/40, loss = 0.4823 - acc = 0.7259\n",
      "  epoch : 6/40, loss = 0.4930 - acc = 0.7323\n",
      "  epoch : 7/40, loss = 0.4918 - acc = 0.7492\n",
      "  epoch : 8/40, loss = 0.4727 - acc = 0.7475\n",
      "  epoch : 9/40, loss = 0.4887 - acc = 0.7399\n",
      "  epoch : 10/40, loss = 0.4980 - acc = 0.7513\n",
      "  epoch : 11/40, loss = 0.4846 - acc = 0.7627\n",
      "  epoch : 12/40, loss = 0.5099 - acc = 0.7690\n",
      "  epoch : 13/40, loss = 0.5375 - acc = 0.7546\n",
      "  epoch : 14/40, loss = 0.5030 - acc = 0.7614\n",
      "  epoch : 15/40, loss = 0.6094 - acc = 0.7753\n",
      "  epoch : 16/40, loss = 0.4833 - acc = 0.7728\n",
      "  epoch : 17/40, loss = 0.4835 - acc = 0.7808\n",
      "  epoch : 18/40, loss = 0.4836 - acc = 0.7893\n",
      "  epoch : 19/40, loss = 0.5702 - acc = 0.7876\n",
      "  epoch : 20/40, loss = 0.4679 - acc = 0.8015\n",
      "  epoch : 21/40, loss = 0.4738 - acc = 0.7973\n",
      "  epoch : 22/40, loss = 0.5693 - acc = 0.7935\n",
      "  epoch : 23/40, loss = 0.5590 - acc = 0.7838\n",
      "  epoch : 24/40, loss = 0.5479 - acc = 0.7741\n",
      "  epoch : 25/40, loss = 0.5604 - acc = 0.7998\n",
      "  epoch : 26/40, loss = 0.7638 - acc = 0.7855\n",
      "  epoch : 27/40, loss = 0.5063 - acc = 0.7808\n",
      "  epoch : 28/40, loss = 0.4807 - acc = 0.8024\n",
      "  epoch : 29/40, loss = 0.4809 - acc = 0.8155\n",
      "  epoch : 30/40, loss = 0.4805 - acc = 0.8218\n",
      "  epoch : 31/40, loss = 0.4907 - acc = 0.8264\n",
      "  epoch : 32/40, loss = 0.4665 - acc = 0.8378\n",
      "  epoch : 33/40, loss = 0.4608 - acc = 0.8391\n",
      "  epoch : 34/40, loss = 0.4972 - acc = 0.8319\n",
      "  epoch : 35/40, loss = 0.4549 - acc = 0.8488\n",
      "  epoch : 36/40, loss = 0.4482 - acc = 0.8505\n",
      "  epoch : 37/40, loss = 0.5235 - acc = 0.8294\n",
      "  epoch : 38/40, loss = 0.4383 - acc = 0.8636\n",
      "  epoch : 39/40, loss = 0.4244 - acc = 0.8775\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Train step\n",
    "    model.train()                                                   # tells to the model you are in training mode (batchnorm and dropout layers work)\n",
    "    for data_tr in trainset:\n",
    "        optim.zero_grad()\n",
    "        X_tr,y_tr = data_tr                                         # unlist the data from the train set\n",
    "        X_tr = X_tr.view(batch_size,3,128,128).float().to(device)     # change the size for the input data - convert to float type\n",
    "        y_tr = y_tr.to(device)\n",
    "        output = model(X_tr)                                        # run the model\n",
    "        loss = criterion(output,y_tr)                               # compute loss\n",
    "        _,pred = output.max(1)                                      # get the index == class of the output along the rows (each sample)\n",
    "        pred_label_train = torch.cat((pred_label_train,pred),dim=0)\n",
    "        true_label_train = torch.cat((true_label_train,y_tr),dim=0)\n",
    "        loss.backward()                                             # compute backpropagation\n",
    "        optim.step()                                                # parameter update\n",
    "\n",
    "    losses.append(loss.cpu().detach().numpy())\n",
    "    acc_t = accuracy_score(true_label_train.cpu(),pred_label_train.cpu())\n",
    "    acc_train.append(acc_t)\n",
    "    print(\"  epoch : {}/{}, loss = {:.4f} - acc = {:.4f}\".format(epoch + 1, num_epochs, loss, acc_t))\n",
    "    if acc_t > best_acc:                                                            # save the best model (the highest accuracy in validation)\n",
    "        torch.save(model.state_dict(),models_trained_path+'CNN_128x128_best_model_trained.pt')\n",
    "        best_acc = acc_t\n",
    "\n",
    "    # Reinitialize the variables to compute accuracy\n",
    "    pred_label_train = torch.empty((0)).to(device)\n",
    "    true_label_train = torch.empty((0)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 709
    },
    "id": "PMDoSLsWxKMo",
    "outputId": "b53fb83a-d19c-43c1-fa8f-3851ecd25212"
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(list(range(num_epochs)), losses)\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(list(range(num_epochs)), acc_train)\n",
    "plt.title(\"Accuracy curve\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpajcpXryE_D"
   },
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "m4xgf1EUyJdU",
    "outputId": "8d338f5e-e2b7-445a-a15f-55d9bc52e75c"
   },
   "outputs": [],
   "source": [
    "model_test = CNN_128x128(input_channel=3,num_classes=n_classes).to(device)                # Initialize a new model\n",
    "model_test.load_state_dict(torch.load(models_trained_path+'CNN_128x128_best_model_trained.pt'))   # Load the model\n",
    "\n",
    "pred_label_test = torch.empty((0,n_classes)).to(device)\n",
    "true_label_test = torch.empty((0)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in testset:\n",
    "    X_te, y_te = data\n",
    "    X_te = X_te.view(batch_size,3,128,128).float().to(device)\n",
    "    y_te = y_te.to(device)\n",
    "    output_test = model_test(X_te)\n",
    "    pred_label_test = torch.cat((pred_label_test,output_test),dim=0)\n",
    "    true_label_test = torch.cat((true_label_test,y_te),dim=0)\n",
    "\n",
    "compute_metrics(y_true=true_label_test,y_pred=pred_label_test,lab_classes=lab_classes)    # function to compute the metrics (accuracy and confusion matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSbuiSe8p1BU"
   },
   "source": [
    "### Visualize the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mW7-EbW5s6qr"
   },
   "outputs": [],
   "source": [
    "# Long to compute for layer 3 and 4\n",
    "plot_weights(model_test.conv1, single_channel = False, collated = True)\n",
    "# plot_weights(model_test.conv2, single_channel = True, collated = False)\n",
    "# plot_weights(model_test.conv3, single_channel = True, collated = False)\n",
    "# plot_weights(model_test.conv4, single_channel = True, collated = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WNH5Olkip4rr",
    "outputId": "496a5b6e-2f8c-41e1-a6fb-4d57d95c23bd"
   },
   "outputs": [],
   "source": [
    "# Get the first kernel from the model\n",
    "kernels_1 = model_test.conv1.weight.data.cpu().clone()\n",
    "visTensor(kernels_1, ch=1, allkernels=False)\n",
    "plt.axis('off')\n",
    "plt.title('kernels from convolutional layer: 1')\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n",
    "# Get the second kernel from the model\n",
    "kernels_2 = model_test.conv2.weight.data.cpu().clone()\n",
    "visTensor(kernels_2, ch=0, allkernels=False)\n",
    "plt.axis('off')\n",
    "plt.title('kernels from convolutional layer: 2')\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n",
    "# Get the second kernel from the model\n",
    "kernels_3 = model_test.conv3.weight.data.cpu().clone()\n",
    "visTensor(kernels_3, ch=0, allkernels=False)\n",
    "plt.axis('off')\n",
    "plt.title('kernels from convolutional layer: 3')\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n",
    "# Get the second kernel from the model\n",
    "kernels_4 = model_test.conv4.weight.data.cpu().clone()\n",
    "visTensor(kernels_4, ch=0, allkernels=False)\n",
    "plt.axis('off')\n",
    "plt.title('kernels from convolutional layer: 4')\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBF8vH5_-0FR"
   },
   "source": [
    "### Visualize features map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BwhLz5RV-2eq",
    "outputId": "c1f76dcc-be2c-4d8b-b29d-91cca8e5dc3c"
   },
   "outputs": [],
   "source": [
    "conv_weights =[]                            # save the weights of convolutional layers\n",
    "conv_layers = []                            # save the convolutional layers\n",
    "model_children = list(model.children())     # get all the model children as list\n",
    "# append all the convolutional layers and their respective wights to the list\n",
    "for i in range(len(model_children)):\n",
    "    if type(model_children[i]) == torch.nn.Conv2d:\n",
    "        conv_weights.append(model_children[i].weight)\n",
    "        conv_layers.append(model_children[i])\n",
    "\n",
    "print(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjpLg08eAy2f"
   },
   "outputs": [],
   "source": [
    "image = X_te[2,:,:,:]\n",
    "original_image = image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FVk1n_rWM73b",
    "outputId": "822f2531-8e4d-4c5e-9258-d693ff5a9259"
   },
   "outputs": [],
   "source": [
    "# process the images through all the convolutional layers \n",
    "outputs = []\n",
    "names = []\n",
    "for layer in conv_layers[0:]:   # run over the convolutional layers\n",
    "    image = layer(image)        # process the image\n",
    "    outputs.append(image)       # save the output of the layer \n",
    "    names.append(str(layer))    # save the name of the layer\n",
    "print(len(outputs))\n",
    "\n",
    "# print feature_maps\n",
    "for feature_map in outputs:\n",
    "    print(feature_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5vQMIurNEsA",
    "outputId": "b77a2c14-dd7a-4e09-b70f-7e61e43cd75a"
   },
   "outputs": [],
   "source": [
    "# Convert from 3D to 2D summing the element for each channel\n",
    "processed = []\n",
    "for feature_map in outputs:\n",
    "    feature_map = feature_map.squeeze(0)\n",
    "    gray_scale = torch.sum(feature_map,0)\n",
    "    gray_scale = gray_scale / feature_map.shape[0]\n",
    "    processed.append(gray_scale.data.cpu().numpy())\n",
    "for fm in processed:\n",
    "    print(fm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "AOShouMpNLLp",
    "outputId": "ed78ebfb-25d9-4de7-af94-5cc1bc684036"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 50))\n",
    "for i in range(len(processed)):\n",
    "    a = fig.add_subplot(5, 4, i+1)\n",
    "    imgplot = plt.imshow(processed[i],cmap='viridis')\n",
    "    a.axis(\"off\")\n",
    "    a.set_title(names[i].split('(')[0], fontsize=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "40wqCwWxNN3Q",
    "outputId": "9235c62e-aaef-4606-e665-55e475ac2a99"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(original_image.reshape(128,128,3).cpu().detach().numpy().astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
